{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kkingo/AndroidApp/blob/master/Gaussian_Belief_Propagation_Library.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdzvHn-JzA-8"
      },
      "source": [
        "# Gaussian Belief Propagation Library\n",
        "\n",
        "Author: [Joseph Ortiz](https://joeaortiz.github.io/)\n",
        "\n",
        "License: [Creative Commons Attribution 4.0 International\n",
        "](https://github.com/gaussianBP/gaussianBP.github.io/blob/master/LICENSE)\n",
        "\n",
        "This library is provided as accompanying code to our article [A visual introduction to Gaussian Belief Propagation\n",
        "](https://gaussianbp.github.io/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzotHENoaY6g",
        "cellView": "form"
      },
      "source": [
        "#@title Imports\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSnr96n2x6TS",
        "cellView": "form"
      },
      "source": [
        "#@title Utility Functions\n",
        "\n",
        "from typing import List, Callable, Optional, Union\n",
        "\n",
        "class Gaussian:\n",
        "    def __init__(self, dim: int, eta: Optional[torch.Tensor]=None, lam: Optional[torch.Tensor]=None, type: torch.dtype = torch.float):\n",
        "        self.dim = dim\n",
        "\n",
        "        if eta is not None and eta.shape == torch.Size([dim]):\n",
        "            self.eta = eta.type(type)\n",
        "        else:\n",
        "            self.eta = torch.zeros(dim, dtype=type)\n",
        "\n",
        "        if lam is not None and lam.shape == torch.Size([dim, dim]):\n",
        "            self.lam = lam.type(type)\n",
        "        else:\n",
        "            self.lam = torch.zeros([dim, dim], dtype=type)\n",
        "\n",
        "    def mean(self) -> torch.Tensor:\n",
        "        return torch.matmul(torch.inverse(self.lam), self.eta)\n",
        "\n",
        "    def cov(self) -> torch.Tensor:\n",
        "        return torch.inverse(self.lam)\n",
        "\n",
        "    def mean_and_cov(self) -> List[torch.Tensor]:\n",
        "        cov = self.cov()\n",
        "        mean = torch.matmul(cov, self.eta)\n",
        "        return [mean, cov]\n",
        "\n",
        "    def set_with_cov_form(self, mean: torch.Tensor, cov: torch.Tensor) -> None:\n",
        "        self.lam = torch.inverse(cov)\n",
        "        self.eta = self.lam @ mean\n",
        "\n",
        "\"\"\"\n",
        "    Defines squared loss functions that correspond to Gaussians. \n",
        "    Robust losses are implemented by scaling the Gaussian covariance.\n",
        "\"\"\"\n",
        "\n",
        "class SquaredLoss(): \n",
        "    def __init__(self, dofs: int, diag_cov: Union[float, torch.Tensor]) -> None:\n",
        "        \"\"\"\n",
        "            dofs: dofs of the measurement\n",
        "            cov: diagonal elements of covariance matrix\n",
        "        \"\"\"\n",
        "        assert diag_cov.shape == torch.Size([dofs])\n",
        "        mat = torch.zeros(dofs, dofs, dtype=diag_cov.dtype)\n",
        "        mat[range(dofs), range(dofs)] = diag_cov\n",
        "        self.cov = mat\n",
        "        self.effective_cov = mat.clone()\n",
        "\n",
        "    def get_effective_cov(self, residual: torch.Tensor) -> None:\n",
        "        \"\"\" Returns the covariance of the Gaussian (squared loss) that matches the loss at the error value. \"\"\"\n",
        "        self.effective_cov = self.cov.clone()\n",
        "\n",
        "    def robust(self) -> bool:\n",
        "        return not torch.equal(self.cov, self.effective_cov)\n",
        "\n",
        "\n",
        "class HuberLoss(SquaredLoss):\n",
        "    def __init__(self, dofs: int, diag_cov: Union[float, torch.Tensor], stds_transition: float) -> None:\n",
        "        \"\"\" \n",
        "            stds_transition: num standard deviations from minimum at which quadratic loss transitions to linear \n",
        "        \"\"\"\n",
        "        SquaredLoss.__init__(self, dofs, diag_cov)\n",
        "        self.stds_transition = stds_transition\n",
        "\n",
        "    def get_effective_cov(self, residual: torch.Tensor) -> None:\n",
        "        mahalanobis_dist = torch.sqrt(residual @ torch.inverse(self.cov) @ residual)\n",
        "        if mahalanobis_dist > self.stds_transition:\n",
        "            self.effective_cov = self.cov * mahalanobis_dist**2 / (2 * self.stds_transition * mahalanobis_dist - self.stds_transition**2)\n",
        "        else:\n",
        "            self.effective_cov = self.cov.clone()\n",
        "\n",
        "\n",
        "class TukeyLoss(SquaredLoss):\n",
        "    def __init__(self, dofs: int, diag_cov: Union[float, torch.Tensor], stds_transition: float) -> None:\n",
        "        \"\"\" \n",
        "            stds_transition: num standard deviations from minimum at which quadratic loss transitions to constant \n",
        "        \"\"\"\n",
        "        SquaredLoss.__init__(self, dofs, diag_cov)\n",
        "        self.stds_transition = stds_transition\n",
        "\n",
        "    def get_effective_cov(self, residual: torch.Tensor) -> None:\n",
        "        mahalanobis_dist = torch.sqrt(residual @ torch.inverse(self.cov) @ residual)\n",
        "        if mahalanobis_dist > self.stds_transition:\n",
        "            self.effective_cov = self.cov * mahalanobis_dist**2 / self.stds_transition**2\n",
        "        else:\n",
        "            self.effective_cov = self.cov.clone()\n",
        "\n",
        "class MeasModel: \n",
        "    def __init__(self, meas_fn: Callable, jac_fn: Callable, loss: SquaredLoss, *args) -> None:\n",
        "        self._meas_fn = meas_fn\n",
        "        self._jac_fn = jac_fn\n",
        "        self.loss = loss\n",
        "        self.args = args\n",
        "        self.linear = True\n",
        "\n",
        "    def jac_fn(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self._jac_fn(x, *self.args)\n",
        "\n",
        "    def meas_fn(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self._meas_fn(x, *self.args)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlWYSDnqxJUW"
      },
      "source": [
        "#@title Main GBP Functions\n",
        "\n",
        "\"\"\"\n",
        "    Defines classes for variable nodes, factor nodes and edges and factor graph.\n",
        "\"\"\"\n",
        "\n",
        "class GBPSettings:\n",
        "    def __init__(self, \n",
        "                 damping: float = 0.,\n",
        "                 beta: float = 0.1,\n",
        "                 num_undamped_iters: int = 5,\n",
        "                 min_linear_iters: int = 10,\n",
        "                 dropout: float = 0.,\n",
        "                 reset_iters_since_relin: List[int] = [],\n",
        "                 type: torch.dtype = torch.float) -> None:\n",
        "\n",
        "        # Parameters for damping the eta component of the message\n",
        "        self.damping = damping \n",
        "        self.num_undamped_iters = num_undamped_iters  # Number of undamped iterations after relinearisation before damping is set to damping\n",
        "\n",
        "        self.dropout = dropout \n",
        "\n",
        "        # Parameters for just in time factor relinearisation\n",
        "        self.beta = beta  # Threshold absolute distance between linpoint and adjacent belief means for relinearisation.\n",
        "        self.min_linear_iters = min_linear_iters  # Minimum number of linear iterations before a factor is allowed to realinearise.\n",
        "        self.reset_iters_since_relin = reset_iters_since_relin\n",
        "\n",
        "    def get_damping(self, iters_since_relin: int) -> float:\n",
        "        if iters_since_relin > self.num_undamped_iters:\n",
        "            return self.damping\n",
        "        else: \n",
        "            return 0.\n",
        "\n",
        "\n",
        "class FactorGraph:\n",
        "    def __init__(self, gbp_settings: GBPSettings = GBPSettings()) -> None:\n",
        "        self.var_nodes = []\n",
        "        self.factors = []\n",
        "        self.gbp_settings = gbp_settings\n",
        "\n",
        "    def add_var_node(self, \n",
        "                    dofs: int, \n",
        "                    prior_mean: Optional[torch.Tensor] = None, \n",
        "                    prior_diag_cov: Optional[Union[float, torch.Tensor]] = None,\n",
        "                    properties: dict = {}) -> None:\n",
        "        variableID = len(self.var_nodes)\n",
        "        self.var_nodes.append(VariableNode(variableID, dofs, properties=properties))\n",
        "        if prior_mean is not None and prior_diag_cov is not None:\n",
        "            prior_cov = torch.zeros(dofs, dofs, dtype=prior_diag_cov.dtype)\n",
        "            prior_cov[range(dofs), range(dofs)] = prior_diag_cov\n",
        "            self.var_nodes[-1].prior.set_with_cov_form(prior_mean, prior_cov)\n",
        "            self.var_nodes[-1].update_belief()\n",
        "            \n",
        "    def add_factor(self, adj_var_ids: List[int], \n",
        "                    measurement: torch.Tensor, \n",
        "                    meas_model: MeasModel,\n",
        "                    properties: dict = {}) -> None:\n",
        "        factorID = len(self.factors)\n",
        "        adj_var_nodes = [self.var_nodes[i] for i in adj_var_ids]\n",
        "        self.factors.append(Factor(factorID, adj_var_nodes, measurement, meas_model, properties=properties))\n",
        "        for var in adj_var_nodes:\n",
        "            var.adj_factors.append(self.factors[-1])\n",
        "\n",
        "    def update_all_beliefs(self) -> None:\n",
        "        for var_node in self.var_nodes:\n",
        "            var_node.update_belief()\n",
        "\n",
        "    def compute_all_messages(self, apply_dropout: bool = True) -> None:\n",
        "        for factor in self.factors:\n",
        "            if apply_dropout and random.random() > self.gbp_settings.dropout or not apply_dropout:\n",
        "                damping = self.gbp_settings.get_damping(factor.iters_since_relin)\n",
        "                factor.compute_messages(damping)\n",
        "\n",
        "    def linearise_all_factors(self) -> None:\n",
        "        for factor in self.factors:\n",
        "            factor.compute_factor()\n",
        "\n",
        "    def robustify_all_factors(self) -> None:\n",
        "        for factor in self.factors:\n",
        "            factor.robustify_loss()\n",
        "\n",
        "    def jit_linearisation(self) -> None:\n",
        "        \"\"\"\n",
        "            Check for all factors that the current estimate is close to the linearisation point. \n",
        "            If not, relinearise the factor distribution.\n",
        "            Relinearisation is only allowed at a maximum frequency of once every min_linear_iters iterations.\n",
        "        \"\"\"\n",
        "        for factor in self.factors:\n",
        "            if not factor.meas_model.linear:\n",
        "                adj_belief_means = factor.get_adj_means()\n",
        "                factor.iters_since_relin += 1\n",
        "                if torch.norm(factor.linpoint - adj_belief_means) > self.gbp_settings.beta and factor.iters_since_relin >= self.gbp_settings.min_linear_iters:\n",
        "                    factor.compute_factor()\n",
        "\n",
        "    def synchronous_iteration(self) -> None:\n",
        "        self.robustify_all_factors()\n",
        "        self.jit_linearisation()  # For linear factors, no compute is done\n",
        "        self.compute_all_messages()\n",
        "        self.update_all_beliefs()\n",
        "\n",
        "    def gbp_solve(self, n_iters: Optional[int] = 20, converged_threshold: Optional[float] = 1e-6, include_priors: bool = True) -> None:\n",
        "        energy_log = [self.energy()]\n",
        "        print(f\"\\nInitial Energy {energy_log[0]:.5f}\")\n",
        "        i = 0\n",
        "        count = 0\n",
        "        not_converged = True\n",
        "        while not_converged and i < n_iters: \n",
        "            self.synchronous_iteration()\n",
        "            if i in self.gbp_settings.reset_iters_since_relin:\n",
        "                for f in self.factors:\n",
        "                    f.iters_since_relin = 1\n",
        "\n",
        "            energy_log.append(self.energy(include_priors=include_priors))\n",
        "            print(\n",
        "                f\"Iter {i+1}  --- \"\n",
        "                f\"Energy {energy_log[-1]:.5f} --- \"\n",
        "                # f\"Belief means: {self.belief_means().numpy()} --- \"\n",
        "                # f\"Robust factors: {[factor.meas_model.loss.robust() for factor in self.factors]}\"\n",
        "                # f\"Relins: {sum([(factor.iters_since_relin==0 and not factor.meas_model.linear) for factor in self.factors])}\"\n",
        "                  )\n",
        "            i += 1\n",
        "            if abs(energy_log[-2] - energy_log[-1]) < converged_threshold:\n",
        "                count += 1\n",
        "                if count == 3:\n",
        "                    not_converged = False\n",
        "            else:\n",
        "                count = 0\n",
        "\n",
        "    def energy(self, eval_point: torch.Tensor = None, include_priors: bool = True) -> float:\n",
        "        \"\"\" Computes the sum of all of the squared errors in the graph using the appropriate local loss function. \"\"\"\n",
        "        if eval_point is None:\n",
        "            energy = sum([factor.get_energy() for factor in self.factors])\n",
        "        else:\n",
        "            var_dofs = torch.tensor([v.dofs for v in self.var_nodes])\n",
        "            var_ix = torch.cat([torch.tensor([0]), torch.cumsum(var_dofs, dim=0)[:-1]])\n",
        "            energy = 0.\n",
        "            for f in self.factors:\n",
        "                local_eval_point = torch.cat([eval_point[var_ix[v.variableID]: var_ix[v.variableID] + v.dofs] for v in f.adj_var_nodes])\n",
        "                energy += f.get_energy(local_eval_point)\n",
        "        if include_priors:\n",
        "            prior_energy = sum([var.get_prior_energy() for var in self.var_nodes])\n",
        "            energy += prior_energy\n",
        "        return energy\n",
        "\n",
        "    def get_joint_dim(self) -> int:\n",
        "        return sum([var.dofs for var in self.var_nodes])\n",
        "\n",
        "    def get_joint(self) -> Gaussian:\n",
        "        \"\"\"\n",
        "            Get the joint distribution over all variables in the information form\n",
        "            If nonlinear factors, it is taken at the current linearisation point.\n",
        "        \"\"\"\n",
        "        dim = self.get_joint_dim()\n",
        "        joint = Gaussian(dim)\n",
        "\n",
        "        # Priors\n",
        "        var_ix = [0] * len(self.var_nodes)\n",
        "        counter = 0\n",
        "        for var in self.var_nodes:\n",
        "            var_ix[var.variableID] = int(counter)\n",
        "            joint.eta[counter:counter + var.dofs] += var.prior.eta\n",
        "            joint.lam[counter:counter + var.dofs, counter:counter + var.dofs] += var.prior.lam\n",
        "            counter += var.dofs\n",
        "\n",
        "        # Other factors\n",
        "        for factor in self.factors:\n",
        "            factor_ix = 0\n",
        "            for adj_var_node in factor.adj_var_nodes:\n",
        "                vID = adj_var_node.variableID\n",
        "                # Diagonal contribution of factor\n",
        "                joint.eta[var_ix[vID]:var_ix[vID] + adj_var_node.dofs] += \\\n",
        "                    factor.factor.eta[factor_ix:factor_ix + adj_var_node.dofs]\n",
        "                joint.lam[var_ix[vID]:var_ix[vID] + adj_var_node.dofs, var_ix[vID]:var_ix[vID] + adj_var_node.dofs] += \\\n",
        "                    factor.factor.lam[factor_ix:factor_ix + adj_var_node.dofs, factor_ix:factor_ix + adj_var_node.dofs]\n",
        "                other_factor_ix = 0\n",
        "                for other_adj_var_node in factor.adj_var_nodes:\n",
        "                    if other_adj_var_node.variableID > adj_var_node.variableID:\n",
        "                        other_vID = other_adj_var_node.variableID\n",
        "                        # Off diagonal contributions of factor\n",
        "                        joint.lam[var_ix[vID]:var_ix[vID] + adj_var_node.dofs, var_ix[other_vID]:var_ix[other_vID] + other_adj_var_node.dofs] += \\\n",
        "                            factor.factor.lam[factor_ix:factor_ix + adj_var_node.dofs, other_factor_ix:other_factor_ix + other_adj_var_node.dofs]\n",
        "                        joint.lam[var_ix[other_vID]:var_ix[other_vID] + other_adj_var_node.dofs, var_ix[vID]:var_ix[vID] + adj_var_node.dofs] += \\\n",
        "                            factor.factor.lam[other_factor_ix:other_factor_ix + other_adj_var_node.dofs, factor_ix:factor_ix + adj_var_node.dofs]\n",
        "                    other_factor_ix += other_adj_var_node.dofs\n",
        "                factor_ix += adj_var_node.dofs\n",
        "\n",
        "        return joint\n",
        "\n",
        "    def MAP(self) -> torch.Tensor:\n",
        "        return self.get_joint().mean()\n",
        "\n",
        "    def dist_from_MAP(self) -> torch.Tensor:\n",
        "        return torch.norm(self.get_joint().mean() - self.belief_means())\n",
        "\n",
        "    def belief_means(self) -> torch.Tensor:\n",
        "        \"\"\" Get an array containing all current estimates of belief means. \"\"\"\n",
        "        return torch.cat([var.belief.mean() for var in self.var_nodes])\n",
        "\n",
        "    def belief_covs(self) -> List[torch.Tensor]:\n",
        "        \"\"\" Get a list containing all current estimates of belief covariances. \"\"\"\n",
        "        covs = [var.belief.cov() for var in self.var_nodes]\n",
        "        return covs\n",
        "\n",
        "    def get_gradient(self, include_priors: bool = True) -> torch.Tensor:\n",
        "        \"\"\" Return gradient wrt the total energy. \"\"\"\n",
        "        dim = self.get_joint_dim()\n",
        "        grad = torch.zeros(dim)\n",
        "        var_dofs = torch.tensor([v.dofs for v in self.var_nodes])\n",
        "        var_ix = torch.cat([torch.tensor([0]), torch.cumsum(var_dofs, dim=0)[:-1]])\n",
        "\n",
        "        if include_priors:\n",
        "            for v in self.var_nodes:\n",
        "                grad[var_ix[v.variableID]:var_ix[v.variableID] + v.dofs] += (v.belief.mean() - v.prior.mean()) @ v.prior.cov()\n",
        "\n",
        "        for f in self.factors:\n",
        "            r = f.get_residual() \n",
        "            jac = f.meas_model.jac_fn(f.linpoint)  # jacobian wrt residual\n",
        "            local_grad = (r @ torch.inverse(f.meas_model.loss.effective_cov) @ jac).flatten()\n",
        "\n",
        "            factor_ix = 0\n",
        "            for adj_var_node in f.adj_var_nodes:\n",
        "                vID = adj_var_node.variableID\n",
        "                grad[var_ix[vID]:var_ix[vID] + adj_var_node.dofs] += local_grad[factor_ix: factor_ix + adj_var_node.dofs]\n",
        "                factor_ix += adj_var_node.dofs\n",
        "        return grad\n",
        "\n",
        "    def gradient_descent_step(self, lr: float = 1e-3) -> None:\n",
        "        grad = self.get_gradient()\n",
        "        i = 0\n",
        "        for v in self.var_nodes:\n",
        "            v.belief.eta = v.belief.lam @ (v.belief.mean() - lr * grad[i: i+v.dofs])\n",
        "            i += v.dofs\n",
        "        self.linearise_all_factors()\n",
        "\n",
        "    def lm_step(self, lambda_lm: float, a: float=1.5, b: float=3) -> bool:\n",
        "        \"\"\" Very close to an LM step, except we always accept update even if it increases the energy. \n",
        "            As to compute the energy if we were to do the update, we would need to relinearise all factors. \n",
        "            Returns lambda parameters for LM. \n",
        "            If lambda_lm = 0, then it is Gauss-Newton.\n",
        "            \"\"\"\n",
        "        current_x = self.belief_means()\n",
        "        initial_energy = self.energy()\n",
        "\n",
        "        joint = self.get_joint()\n",
        "        A = joint.lam + lambda_lm * torch.eye(len(joint.eta))\n",
        "        b_mat = -self.get_gradient()\n",
        "        delta_x = torch.inverse(A) @ b_mat\n",
        "\n",
        "        i = 0  # apply update\n",
        "        for v in self.var_nodes:\n",
        "            v.belief.eta = v.belief.lam @ (v.belief.mean() + delta_x[i: i+v.dofs])\n",
        "            i += v.dofs\n",
        "        self.linearise_all_factors()\n",
        "        new_energy = self.energy()\n",
        "\n",
        "        if lambda_lm == 0.:  # Gauss-Newton\n",
        "            return lambda_lm\n",
        "        if new_energy < initial_energy:  # accept update \n",
        "            lambda_lm /= a\n",
        "            return lambda_lm\n",
        "        else:  # undo update\n",
        "            i = 0  # apply update\n",
        "            for v in self.var_nodes:\n",
        "                v.belief.eta = v.belief.lam @ (v.belief.mean() - delta_x[i: i+v.dofs])\n",
        "                i += v.dofs\n",
        "            self.linearise_all_factors()\n",
        "            lambda_lm = min(lambda_lm*b, 1e5)\n",
        "            return lambda_lm\n",
        "\n",
        "    def print(self, brief=False) -> None:\n",
        "        print(\"\\nFactor Graph:\")\n",
        "        print(f\"# Variable nodes: {len(self.var_nodes)}\")\n",
        "        if not brief:\n",
        "            for i, var in enumerate(self.var_nodes):\n",
        "                print(f\"Variable {i}: connects to factors {[f.factorID for f in var.adj_factors]}\")\n",
        "                print(f\"    dofs: {var.dofs}\")\n",
        "                print(f\"    prior mean: {var.prior.mean().numpy()}\")\n",
        "                print(f\"    prior covariance: diagonal sigma {torch.diag(var.prior.cov()).numpy()}\")\n",
        "        print(f\"# Factors: {len(self.factors)}\")\n",
        "        if not brief:\n",
        "            for i, factor in enumerate(self.factors):\n",
        "                if factor.meas_model.linear:\n",
        "                    print(\"Linear\", end =\" \")\n",
        "                else: \n",
        "                    print(\"Nonlinear\", end =\" \")\n",
        "                print(f\"Factor {i}: connects to variables {factor.adj_vIDs}\")\n",
        "                print(f\"    measurement model: {type(factor.meas_model).__name__},\"\n",
        "                    f\" {type(factor.meas_model.loss).__name__},\"\n",
        "                    f\" diagonal sigma {torch.diag(factor.meas_model.loss.effective_cov).detach().numpy()}\")\n",
        "                print(f\"    measurement: {factor.measurement.numpy()}\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "\n",
        "class VariableNode:\n",
        "    def __init__(self, id: int, dofs: int, properties: dict = {}) -> None:\n",
        "        self.variableID = id\n",
        "        self.properties = properties\n",
        "        self.dofs = dofs\n",
        "        self.adj_factors = []\n",
        "        self.belief = Gaussian(dofs)\n",
        "        self.prior = Gaussian(dofs)  # prior factor, implemented as part of variable node\n",
        "\n",
        "    def update_belief(self) -> None:\n",
        "        \"\"\" Update local belief estimate by taking product of all incoming messages along all edges. \"\"\"\n",
        "        self.belief.eta = self.prior.eta.clone()  # message from prior factor\n",
        "        self.belief.lam = self.prior.lam.clone()\n",
        "        for factor in self.adj_factors:  # messages from other adjacent variables\n",
        "            message_ix = factor.adj_vIDs.index(self.variableID)\n",
        "            self.belief.eta += factor.messages[message_ix].eta\n",
        "            self.belief.lam += factor.messages[message_ix].lam\n",
        "\n",
        "    def get_prior_energy(self) -> float:\n",
        "        energy = 0.\n",
        "        if self.prior.lam[0, 0] != 0.:\n",
        "            residual = self.belief.mean() - self.prior.mean()\n",
        "            energy += 0.5 * residual @ self.prior.lam @ residual\n",
        "        return energy\n",
        "\n",
        "\n",
        "class Factor:\n",
        "    def __init__(self,\n",
        "                 id: int,\n",
        "                 adj_var_nodes: List[VariableNode],\n",
        "                 measurement: torch.Tensor,\n",
        "                 meas_model: MeasModel,\n",
        "                 type: torch.dtype = torch.float,\n",
        "                 properties: dict = {}) -> None:\n",
        "\n",
        "        self.factorID = id\n",
        "        self.properties = properties\n",
        "\n",
        "        self.adj_var_nodes = adj_var_nodes\n",
        "        self.dofs = sum([var.dofs for var in adj_var_nodes])\n",
        "        self.adj_vIDs = [var.variableID for var in adj_var_nodes]\n",
        "        self.messages = [Gaussian(var.dofs) for var in adj_var_nodes]\n",
        "\n",
        "        self.factor = Gaussian(self.dofs)\n",
        "        self.linpoint = torch.zeros(self.dofs, dtype=type)\n",
        "\n",
        "        self.measurement = measurement\n",
        "        self.meas_model = meas_model\n",
        "\n",
        "        # For smarter GBP implementations\n",
        "        self.iters_since_relin = 0\n",
        "\n",
        "        self.compute_factor()\n",
        "\n",
        "    def get_adj_means(self) -> torch.Tensor:\n",
        "        adj_belief_means = [var.belief.mean() for var in self.adj_var_nodes]\n",
        "        return torch.cat(adj_belief_means)\n",
        "\n",
        "    def get_residual(self, eval_point: torch.Tensor = None) -> torch.Tensor:\n",
        "        \"\"\" Compute the residual vector. \"\"\"\n",
        "        if eval_point is None:\n",
        "            eval_point = self.get_adj_means()\n",
        "        return self.meas_model.meas_fn(eval_point) - self.measurement\n",
        "\n",
        "    def get_energy(self, eval_point: torch.Tensor = None) -> float:\n",
        "        \"\"\" Computes the squared error using the appropriate loss function. \"\"\"\n",
        "        residual = self.get_residual(eval_point)\n",
        "        # print(\"adj_belifes\", self.get_adj_means())\n",
        "        # print(\"pred and meas\", self.meas_model.meas_fn(self.get_adj_means()), self.measurement)\n",
        "        # print(\"residual\", self.get_residual(), self.meas_model.loss.effective_cov)\n",
        "        return 0.5 * residual @ torch.inverse(self.meas_model.loss.effective_cov) @ residual\n",
        "\n",
        "    def robust(self) -> bool:\n",
        "        return self.meas_model.loss.robust()\n",
        "\n",
        "    def compute_factor(self) -> None:\n",
        "        \"\"\"\n",
        "            Compute the factor at current adjacente beliefs using robust.\n",
        "            If measurement model is linear then factor will always be the same regardless of linearisation point.\n",
        "        \"\"\"\n",
        "        self.linpoint = self.get_adj_means()\n",
        "        J = self.meas_model.jac_fn(self.linpoint)\n",
        "        pred_measurement = self.meas_model.meas_fn(self.linpoint)\n",
        "        self.meas_model.loss.get_effective_cov(pred_measurement - self.measurement)\n",
        "        effective_lam = torch.inverse(self.meas_model.loss.effective_cov)\n",
        "        self.factor.lam = J.T @ effective_lam @ J\n",
        "        self.factor.eta = ((J.T @ effective_lam) @ (J @ self.linpoint + self.measurement - pred_measurement)).flatten()\n",
        "        self.iters_since_relin = 0\n",
        "\n",
        "    def robustify_loss(self) -> None:\n",
        "        \"\"\"\n",
        "            Rescale the variance of the noise in the Gaussian measurement model if necessary and update the factor\n",
        "            correspondingly.\n",
        "        \"\"\"\n",
        "        old_effective_cov = self.meas_model.loss.effective_cov[0, 0]\n",
        "        self.meas_model.loss.get_effective_cov(self.get_residual())\n",
        "        self.factor.eta *= old_effective_cov / self.meas_model.loss.effective_cov[0, 0]\n",
        "        self.factor.lam *= old_effective_cov / self.meas_model.loss.effective_cov[0, 0]\n",
        "\n",
        "    def compute_messages(self, damping: float = 0.) -> None:\n",
        "        \"\"\" Compute all outgoing messages from the factor. \"\"\"\n",
        "        messages_eta, messages_lam = [], []\n",
        "        \n",
        "        start_dim = 0\n",
        "        for v in range(len(self.adj_vIDs)):\n",
        "            eta_factor, lam_factor = self.factor.eta.clone().double(), self.factor.lam.clone().double()\n",
        "\n",
        "            # Take product of factor with incoming messages\n",
        "            start = 0\n",
        "            for var in range(len(self.adj_vIDs)):\n",
        "                if var != v:\n",
        "                    var_dofs = self.adj_var_nodes[var].dofs\n",
        "                    eta_factor[start:start + var_dofs] += self.adj_var_nodes[var].belief.eta - self.messages[var].eta\n",
        "                    lam_factor[start:start + var_dofs, start:start + var_dofs] += self.adj_var_nodes[var].belief.lam - self.messages[var].lam\n",
        "                start += self.adj_var_nodes[var].dofs\n",
        "\n",
        "            # Divide up parameters of distribution\n",
        "            mess_dofs = self.adj_var_nodes[v].dofs\n",
        "            eo = eta_factor[start_dim:start_dim + mess_dofs]\n",
        "            eno = torch.cat((eta_factor[:start_dim], eta_factor[start_dim + mess_dofs:]))\n",
        "\n",
        "            loo = lam_factor[start_dim:start_dim + mess_dofs, start_dim:start_dim + mess_dofs]\n",
        "            lono = torch.cat((lam_factor[start_dim:start_dim + mess_dofs, :start_dim],\n",
        "                              lam_factor[start_dim:start_dim + mess_dofs, start_dim + mess_dofs:]), dim=1)\n",
        "            lnoo = torch.cat((lam_factor[:start_dim, start_dim:start_dim + mess_dofs],\n",
        "                              lam_factor[start_dim + mess_dofs:, start_dim:start_dim + mess_dofs]), dim=0)\n",
        "            lnono = torch.cat(\n",
        "                        (\n",
        "                            torch.cat((lam_factor[:start_dim, :start_dim], lam_factor[:start_dim, start_dim + mess_dofs:]), dim=1),\n",
        "                            torch.cat((lam_factor[start_dim + mess_dofs:, :start_dim], lam_factor[start_dim + mess_dofs:, start_dim + mess_dofs:]), dim=1)\n",
        "                        ),\n",
        "                        dim=0 \n",
        "                    )\n",
        "\n",
        "            new_message_lam = loo - lono @ torch.inverse(lnono) @ lnoo\n",
        "            new_message_eta = eo - lono @ torch.inverse(lnono) @ eno\n",
        "            messages_eta.append((1 - damping) * new_message_eta + damping * self.messages[v].eta)\n",
        "            messages_lam.append((1 - damping) * new_message_lam + damping * self.messages[v].lam)\n",
        "            start_dim += self.adj_var_nodes[v].dofs\n",
        "\n",
        "        for v in range(len(self.adj_vIDs)):\n",
        "            self.messages[v].lam = messages_lam[v]\n",
        "            self.messages[v].eta = messages_eta[v]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpQngvFUqBLW"
      },
      "source": [
        "#1D Line Fitting Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiAOHWV4uMGY"
      },
      "source": [
        "#@title Create Custom factors\n",
        "\n",
        "def height_meas_fn(x: torch.Tensor, gamma: torch.Tensor):\n",
        "    J = torch.tensor([[1-gamma, gamma]])\n",
        "    return J @ x\n",
        "\n",
        "def height_jac_fn(x: torch.Tensor, gamma: torch.Tensor):\n",
        "    return torch.tensor([[1-gamma, gamma]])\n",
        "\n",
        "class HeightMeasurementModel(MeasModel):\n",
        "    def __init__(self, loss: SquaredLoss, gamma: torch.Tensor) -> None:\n",
        "        MeasModel.__init__(self, height_meas_fn, height_jac_fn, loss, gamma)\n",
        "        self.linear = True\n",
        "\n",
        "def smooth_meas_fn(x: torch.Tensor):\n",
        "    return torch.tensor([x[1] - x[0]])\n",
        "\n",
        "def smooth_jac_fn(x: torch.Tensor):\n",
        "    return torch.tensor([[-1., 1.]])\n",
        "\n",
        "class SmoothingModel(MeasModel):\n",
        "    def __init__(self, loss: SquaredLoss) -> None:\n",
        "        MeasModel.__init__(self, smooth_meas_fn, smooth_jac_fn, loss)\n",
        "        self.linear = True\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6K9LlHuZqEy2"
      },
      "source": [
        "#@title Set parameters\n",
        "n_varnodes = 20\n",
        "x_range = 10\n",
        "n_measurements = 15\n",
        "\n",
        "gbp_settings = GBPSettings(\n",
        "    damping = 0.1,\n",
        "    beta = 0.01,\n",
        "    num_undamped_iters = 1,\n",
        "    min_linear_iters = 1,\n",
        "    dropout = 0.0,\n",
        "  )\n",
        "\n",
        "# Gaussian noise measurement model parameters:\n",
        "prior_cov = torch.tensor([10.])\n",
        "data_cov = torch.tensor([0.05]) \n",
        "smooth_cov = torch.tensor([0.1])\n",
        "data_std = torch.sqrt(data_cov)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "IMJW-KoHqduY",
        "outputId": "5852a5bc-42e9-479a-d383-1f846c6fe4a0"
      },
      "source": [
        "#@title Create measurements {vertical-output: true}\n",
        "\n",
        "# Plot measurements\n",
        "meas_x = torch.rand(n_measurements)*x_range\n",
        "meas_y = torch.sin(meas_x) + torch.normal(0, torch.full([n_measurements], data_std.item()))\n",
        "plt.scatter(meas_x, meas_y, color=\"red\", label=\"Measurements\", marker=\".\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "id": "xMu4I5pgsX5u",
        "outputId": "f39af793-323d-4124-814b-1768c96e0495"
      },
      "source": [
        "#@title Create factor graph {vertical-output: true}\n",
        "fg = FactorGraph(gbp_settings)\n",
        "\n",
        "xs = torch.linspace(0, x_range, n_varnodes).float().unsqueeze(0).T\n",
        "for i in range(n_varnodes):\n",
        "    fg.add_var_node(1, torch.tensor([0.]), prior_cov)\n",
        "\n",
        "for i in range(n_varnodes-1):\n",
        "    fg.add_factor(\n",
        "      [i, i+1], \n",
        "      torch.tensor([0.]), \n",
        "      SmoothingModel(SquaredLoss(1, smooth_cov))\n",
        "      )\n",
        "\n",
        "for i in range(n_measurements):\n",
        "    ix2 = np.argmax(xs > meas_x[i])\n",
        "    ix1 = ix2 - 1\n",
        "    gamma = (meas_x[i] - xs[ix1]) / (xs[ix2] - xs[ix1])\n",
        "    fg.add_factor(\n",
        "      [ix1, ix2], \n",
        "      meas_y[i], \n",
        "      HeightMeasurementModel(\n",
        "          SquaredLoss(1, data_cov), \n",
        "          gamma  \n",
        "        )\n",
        "      )\n",
        "\n",
        "fg.print(brief=True)\n",
        "\n",
        "#@markdown Beliefs are initialized to zero\n",
        "# Plot beliefs and measurements\n",
        "covs = torch.sqrt(torch.cat(fg.belief_covs()).flatten())\n",
        "plt.errorbar(xs, fg.belief_means(), yerr=covs, fmt='o', color=\"C0\", label='Beliefs')\n",
        "plt.scatter(meas_x, meas_y, color=\"red\", label=\"Measurements\", marker=\".\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Factor Graph:\n",
            "# Variable nodes: 20\n",
            "# Factors: 34\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/__init__.py:959: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
            "  x = np.asarray(x)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/__init__.py:959: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  x = np.asarray(x)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/collections.py:1368: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
            "  seg = np.asarray(seg, float)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py:2027: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
            "  xys = np.asarray(xys)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py:2027: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  xys = np.asarray(xys)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 965
        },
        "id": "Y4gutLjG5_-L",
        "outputId": "2e8f5173-59bc-4ee7-f891-53e473a6c252"
      },
      "source": [
        "#@title Solve with GBP {vertical-output: true}\n",
        "fg.gbp_solve(n_iters=50)\n",
        "\n",
        "# Plot beliefs and measurements\n",
        "covs = torch.sqrt(torch.cat(fg.belief_covs()).flatten())\n",
        "plt.errorbar(xs, fg.belief_means(), yerr=covs, fmt='o', color=\"C0\", label='Beliefs')\n",
        "plt.scatter(meas_x, meas_y, color=\"red\", label=\"Measurements\", marker=\".\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Initial Energy 81.98669\n",
            "Iter 1  --- Energy 29.00060 --- \n",
            "Iter 2  --- Energy 22.04576 --- \n",
            "Iter 3  --- Energy 12.92844 --- \n",
            "Iter 4  --- Energy 11.69807 --- \n",
            "Iter 5  --- Energy 10.74326 --- \n",
            "Iter 6  --- Energy 10.38428 --- \n",
            "Iter 7  --- Energy 10.34638 --- \n",
            "Iter 8  --- Energy 10.30881 --- \n",
            "Iter 9  --- Energy 10.31084 --- \n",
            "Iter 10  --- Energy 10.30814 --- \n",
            "Iter 11  --- Energy 10.30247 --- \n",
            "Iter 12  --- Energy 10.30023 --- \n",
            "Iter 13  --- Energy 10.30016 --- \n",
            "Iter 14  --- Energy 10.29975 --- \n",
            "Iter 15  --- Energy 10.29978 --- \n",
            "Iter 16  --- Energy 10.29975 --- \n",
            "Iter 17  --- Energy 10.29972 --- \n",
            "Iter 18  --- Energy 10.29971 --- \n",
            "Iter 19  --- Energy 10.29971 --- \n",
            "Iter 20  --- Energy 10.29970 --- \n",
            "Iter 21  --- Energy 10.29970 --- \n",
            "Iter 22  --- Energy 10.29970 --- \n",
            "Iter 23  --- Energy 10.29970 --- \n",
            "Iter 24  --- Energy 10.29970 --- \n",
            "Iter 25  --- Energy 10.29970 --- \n",
            "Iter 26  --- Energy 10.29970 --- \n",
            "Iter 27  --- Energy 10.29970 --- \n",
            "Iter 28  --- Energy 10.29970 --- \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/__init__.py:959: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
            "  x = np.asarray(x)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/__init__.py:959: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  x = np.asarray(x)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/collections.py:1368: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
            "  seg = np.asarray(seg, float)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py:2027: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
            "  xys = np.asarray(xys)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py:2027: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  xys = np.asarray(xys)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def meas_fn(x):\n",
        "    length = int(x.shape[0] / 2)\n",
        "    J = torch.cat((-torch.eye(length), torch.eye(length)), dim=1)\n",
        "    return J @ x\n",
        "\n",
        "\n",
        "def jac_fn(x):\n",
        "    length = int(x.shape[0] / 2)\n",
        "    return torch.cat((-torch.eye(length), torch.eye(length)), dim=1)\n",
        "\n",
        "\n",
        "class LinearDisplacementModel(MeasModel):\n",
        "    def __init__(self, loss: SquaredLoss) -> None:\n",
        "        MeasModel.__init__(self, meas_fn, jac_fn, loss)\n",
        "        self.linear = True\n",
        "\n"
      ],
      "metadata": {
        "id": "WjyI3nmpdUxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fg = FactorGraph(gbp_settings)\n",
        "\n",
        "# Initialize variable nodes for frames with prior\n",
        "for i in range(size):\n",
        "    for j in range(size):\n",
        "        init = torch.FloatTensor([j, i]) + torch.normal(torch.zeros(2), prior_noise_std)\n",
        "        sigma = prior_sigma\n",
        "        if i == 0 and j == 0:\n",
        "            init = torch.FloatTensor([j, i])\n",
        "            sigma = torch.tensor([0.001, 0.001])\n",
        "        print(init, sigma)\n",
        "        fg.add_var_node(2, init, sigma)\n",
        "\n",
        "for i in range(size):\n",
        "    for j in range(size):\n",
        "        if j < size - 1:\n",
        "            fg.add_factor(\n",
        "                [i*size + j, i*size + j + 1],\n",
        "                torch.tensor([1., 0.]) + torch.normal(torch.zeros(2), torch.sqrt(noise_cov[0])),\n",
        "                CubedDisplacementModel(SquaredLoss(dim, noise_cov))\n",
        "            )\n",
        "        if i < size - 1:\n",
        "            fg.add_factor(\n",
        "                [i*size + j, (i+1)*size + j],\n",
        "                torch.tensor([0., 1.]) + torch.normal(torch.zeros(2), torch.sqrt(noise_cov[0])),\n",
        "                CubedDisplacementModel(SquaredLoss(dim, noise_cov))\n",
        "            )\n",
        "\n",
        "fg = gbp.FactorGraph(gbp_settings)\n",
        "\n",
        "anchor_prior_diag_cov = torch.tensor([1.])\n",
        "prior_diag_cov = torch.tensor([10.])\n",
        "fg.add_var_node(1, torch.tensor([0.]), anchor_prior_diag_cov)\n",
        "fg.add_var_node(1, torch.tensor([1.2]), prior_diag_cov)\n",
        "fg.add_var_node(1, torch.tensor([3.2]), prior_diag_cov)\n",
        "\n",
        "\n",
        "meas_dofs = 1\n",
        "noise_diag_cov = torch.tensor([1.])\n",
        "transition_stds = 0.5\n",
        "\n",
        "# fg.add_factor([0, 1], torch.tensor([1.]), LinearDisplacementModel(HuberLoss(meas_dofs, noise_diag_cov, transition_stds)))\n",
        "# fg.add_factor([1, 2], torch.tensor([1.]), LinearDisplacementModel(HuberLoss(meas_dofs, noise_diag_cov, transition_stds)))\n",
        "fg.add_factor([0, 1], torch.tensor([1.]), LinearDisplacementModel(SquaredLoss(meas_dofs, noise_diag_cov)))\n",
        "fg.add_factor([1, 2], torch.tensor([1.]), LinearDisplacementModel(SquaredLoss(meas_dofs, noise_diag_cov)))\n",
        "\n",
        "fg.print()\n",
        "\n",
        "print(\"Initial belief means\", fg.belief_means().numpy())\n",
        "\n",
        "joint = fg.get_joint()\n",
        "print(\"MAP: \", fg.MAP().numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "t_0rULy9eQpS",
        "outputId": "79acbcde-9d72-4d07-a0b1-2cd1c904bf9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-d8c1684fe834>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Initialize variable nodes for frames with prior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0minit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior_noise_std\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'size' is not defined"
          ]
        }
      ]
    }
  ]
}